---
title: "HW01"
author: "caojilin"
date: "9/5/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
####Problem 1d)
```{r}
dat = read.table("PearsonHeightData.txt", header = T)
lmod1 = lm(Son ~ Father, data = dat)
lmod2 = lm(Father ~ Son, data = dat)
plot(Son ~ Father, data = dat)
abline(lmod1, col = "blue")
abline(lmod2, col = "red")
```

####Problem 5
```{r}
library(datasets)
a <- anscombe
par(mfrow=c(2,2))


lm1 = lm(a$y1 ~ a$x1)

lm2 = lm(a$y2 ~ a$x2)

lm3 = lm(a$y3 ~ a$x3)

lm4 = lm(a$y4 ~ a$x4)

plot(a$x1,a$y1, main=paste("Dataset One"))
abline(lm1,col="red")
plot(a$x2,a$y2, main=paste("Dataset Two"))
abline(lm2,col="red")
plot(a$x3,a$y3, main=paste("Dataset Three"))
abline(lm3,col="red")
plot(a$x4,a$y4, main=paste("Dataset Four"))
abline(lm4,col="red")
```

dataset 1, and 3 make sense. Although dataset 3 has a outlier, it doesn't matter.  
dataset 2 looks like a parabola, linear model is not accurate as dataset gets larger, but it's ok for small dataset like this.  
dataset 4 doesn't make sense because most of the data are clustered on the line x = 8  

predictions are:
```{r}
lm1$coefficients[1] + 10 * lm1$coefficients[2]
lm2$coefficients[1] + 10 * lm2$coefficients[2]
lm3$coefficients[1] + 10 * lm3$coefficients[2]
lm4$coefficients[1] + 10 * lm4$coefficients[2]
```
again, predictions for dataset 1,2,3 are close to real data values. So they make sense.
but dataset 4 doesn't make sense.

####Problem 7
###a)

we assume simple linear regression model for $$(x_1,y_1),(x_2,y_2)\dots(x_n,y_n)$$ to be $$y_i = \beta_0 + \beta_1 x_1 + e_i$$ where $$\mathop{\mathbb{E}}(e_i\mid X_i) = 0$$ That means we have to make sure $$\mathop{\mathbb{E}}(e_i\mid X_i) = 0$$ in order to assume a linear model.  
To verify that: for $$x_i<=65$$ $$y_i$$ can be wirrten as $$y_i = N(\beta_0 +\beta_1 x_i,25) + N(0,25)$$ where $$e_i = N(0,25)$$ this satisfies that $$\mathop{\mathbb{E}}(e_i\mid X_i) = 0$$  
similarly, for $$ 65<x_i<=70$$, $$y_i = \beta_0 + \beta_1 x_i + e_i$$ where $$e_i = 10T_i$$ whose mean is 0    
for $$ x_i > 70$$, $$y_i = \beta_0 + \beta_1 x_i + e_i$$ where $$e_i =U_i$$, and $$\mathop{\mathbb{E}}(U_i) = 0$$    
Therefore, the condition $$\mathop{\mathbb{E}}(e_i\mid X_i) = 0$$ for simple linear regression model is satisfied. Since we have proved in the class and in the homework that lease square estimators are unbiased. we can conclude that $$\hat{\beta_0}$$ and $$\hat{\beta_1}$$ are unbiased.

###b)
```{r}
M = 1:10000

samp = function(){
  x = seq(59, 76, length.out = 100)
  x1=x[x<=65]
  y1 = rnorm(length(x1),mean = 32+0.5*x1,sd = 25)
  x2=x[x>65 & x<=70]
  y2 = 32+0.5*x2+10*rt(n = length(x2),df = 3)
  x3=x[x>70]
  y3=32+0.5*x3+runif(length(x3),min = -8,max = 8)
  y = c(y1,y2,y3)
  return(list(x,y))
}
beta0_hat = c()
beta1_hat = c()
sd_beta0 = c()
sd_beta1 = c()

for (i in M) {
  sample = samp()
  x = sample[[1]]
  y = sample[[2]]
  lmod = lm(y ~ x)
  beta0_hat = c(beta0_hat, lmod$coefficients[1])
  beta1_hat = c(beta1_hat, lmod$coefficients[2])
  sd_beta0 = c(sd_beta0,summary(lmod)$coefficients[3])
  sd_beta1 = c(sd_beta1,summary(lmod)$coefficients[4])

}

hist(beta0_hat)
abline(v=32,col="red")
hist(beta1_hat)
abline(v=0.5,col="red")

#bias of beta 0
sum(beta0_hat -32)/10000

#bias of beta 1
sum(beta1_hat -0.5)/10000
```
From histogram, the estimates of the bias are close to 0 and we found that the center for both graph are close to real $$\beta_0=32$$ and $$\beta_1=0.5$$ This verifies unbiasedness  

###c)

homoskedasticity means that same  variance of the errors, $$Var(e_i \mid X)  =  \sigma^2$$ for  each i. However, in this problem, we see that $$Var(e_i \mid X)  =  \sigma^2$$ are not same for each i. Thus, homoskedasticity is not valid here.

###d)
we would like to check unbiasedness of $$\mathop{\mathbb{E}}(\hat{Var(\hat{\beta})}) = Var(\hat{\beta})$$
```{r}
sd0 = sd(beta0_hat)
sd1 = sd(beta1_hat)
hist(sd_beta0,breaks = 100)
abline(v=sd0,col="red")
hist(sd_beta1,breaks=100)
abline(v=sd1,col="red")
```

As we see from histograms, the centers are not close to sd0 and sd1, meaning that $$\mathop{\mathbb{E}}(\hat{Var(\hat{\beta})}) \neq Var(\hat{\beta})$$ this verifies that homoskedasticity is not valid



