---
title: "HW05"
author: "caojilin"
date: "10/24/2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("model4you")
library(ggplot2)
library(faraway)
library(leaps)
library(car)
body = read.csv("Bodyfat.csv")
#check skewness
library(e1071)
ant = read.csv("thatch-ant.dat.txt")
```

###Problem 1  
#####When I run this code, R gives me four plots. Describe each of these plots and explain how to interpret them.(0.75 points)
```{r}
g = lm(bodyfat ~ Age +Weight + Height+Thigh,data=body)
par(mfrow = c(2, 3))
plot(g)
```

First is the residual against fitted values plot. We can use it to check assumption of linearity and the constant variance. The constant variance assupmtion is true if we see there is no pattern for residuals and they are equally spread around x-axis. Linearity assumption fails if we see very large residuals. We expect to see residuals that are not far away from 0. We also expect the red line to be linear. But we see that there are some points pull the red line away from y=0. These are potential outliers.

Second is qqplot. sorted values $x_i$ vs $\Phi^{-1}(\frac{i}{n})$ We can use it to check the normality assupmtion of the error terms. If we observe a stright line, then our assumption is true. We see left tail is heavier than normal. Those points are potential outliers.

Third is scale-location plot. It's square rooted standardized residual vs. predicted value. This is useful for checking the assumption of homoscedasticity. We are checking to see if there is a pattern in the residuals. Similar to the first graph. Some data points pull the red line up. Those are potential outliers.

Fourth is the residaul vs leverage. The cook's distance is interpreted by the red contours. It is a measure of the influence of each observation on the regression coefficients. It measures the distance between $\hat{\beta} \ and\  \hat{\beta_{[i]}}$. Any observation for which the cook's distance is substantially larger than other Cook's distances requires investigation.



###Problem 2

#####a$)$ Using each of the following methods, perform variable selection to select a subset of the explanatory variables for modeling the response: (0.25 points each = 1.5 points)
i. Backward elimination using the individual p-values.
```{r,results='hide'}
lmod2 = lm(bodyfat ~ Age + Weight + Height + Neck + Chest + Abdomen+ Hip +
             Thigh+ Knee + Ankle + Biceps + Forearm+ Wrist, data=body)
#cirtical value = 0.15
summary(lmod2)
lmod2 = update(lmod2,. ~. -Knee)
summary(lmod2)
lmod2 = update(lmod2,. ~. -Chest)
summary(lmod2)
lmod2 = update(lmod2,. ~. -Height)
summary(lmod2)
lmod2 = update(lmod2,. ~. -Ankle)
summary(lmod2)
lmod2 = update(lmod2,. ~. -Biceps)
summary(lmod2)
lmod2 = update(lmod2,. ~. -Hip)
summary(lmod2)
#Age Weight Neck Abdomen Thigh Forearm  Wrist 
lmod2.backward = lmod2 
```

ii. Forward Selection using p-values.
```{r,results='hide'}
null=lm(bodyfat ~ 1, body)
full = lm(bodyfat ~ Age + Weight + Height + Neck + Chest + Abdomen+ Hip +
             Thigh+ Knee + Ankle + Biceps + Forearm+ Wrist, data=body)
step(null, scope=list(lower=null, upper=full), direction="forward")
lmod2.forward = lm(bodyfat ~Abdomen+Weight+Wrist+Forearm+Neck+Age+Thigh,data=body)
```

iii. Adjusted R2
```{r}
b<-regsubsets(bodyfat~Age + Weight + Height + Neck + Chest + Abdomen+ Hip +
             Thigh+ Knee + Ankle + Biceps + Forearm+ Wrist,data=body)
rs = summary(b)
#we select the model with highest adjusted R-squares
rs$which[which(rs$adjr2 == max(rs$adjr2)),]
lmod2.adjr2 = lm(bodyfat ~ Age+Weight+Neck+Abdomen+Hip+Thigh+Forearm+Wrist, body)
# plot(b,scale="adjr2")
```
iv. AIC
```{r,results='hide'}
lmod2 = lm(bodyfat ~ Age + Weight + Height + Neck + Chest + Abdomen+ Hip +
             Thigh+ Knee + Ankle + Biceps + Forearm+ Wrist, data=body)
step(lmod2)
lmod2.AIC = lm(formula = bodyfat ~ Age + Weight + Neck + Abdomen + Hip +
    Thigh + Forearm + Wrist, data = body)
```

v. BIC
```{r,results='hide'}
lmod2 = lm(bodyfat ~ Age + Weight + Height + Neck + Chest + Abdomen+ Hip +
             Thigh+ Knee + Ankle + Biceps + Forearm+ Wrist, data=body)
step(lmod2,k = log(nrow(body)))
lmod2.BIC = lm(formula = bodyfat ~ Weight + Abdomen + Forearm + Wrist, data = body)
```

vi. Mallow's Cp
```{r}
rs$which
rs$cp
#row 7 has smallest cp
lmod2.cp = lm(bodyfat ~ Age + Weight+Neck+Abdomen+Thigh+Forearm+Wrist , data=body)
```


#####b$)$ Let M1,...,M6 denote the six models selected by each of the six variable selection methods of the previous part. Select one of these models by cross-validation. (0.2 points)
```{r}
lmod2.backward.score = sum((lmod2.backward$residuals/(1 - influence(lmod2.backward)$hat))^2)
lmod2.forward.score = sum((lmod2.forward$residuals/(1 - influence(lmod2.forward)$hat))^2)
lmod2.adjr2.score = sum((lmod2.adjr2$residuals/(1 - influence(lmod2.adjr2)$hat))^2)
lmod2.AIC.score = sum((lmod2.AIC$residuals/(1 - influence(lmod2.AIC)$hat))^2)
lmod2.BIC.score = sum((lmod2.BIC$residuals/(1 - influence(lmod2.BIC)$hat))^2)
lmod2.cp.score = sum((lmod2.cp$residuals/(1-influence(lmod2.cp)$hat))^2)
score = c(lmod2.backward.score,lmod2.forward.score,lmod2.adjr2.score,lmod2.AIC.score,lmod2.BIC.score,lmod2.cp.score)
score
```
we notice that both AIC and adjusted R-square have the same score and they selected the same variables. So choose either one.

#####c$)$ Let M be the model selected in the previous part. Fit this model to the data. Perform regression diagnostics. Comment on the validity of the assumptions of the linear model. Identify infuential observations and outliers. Delete them if necessary and re-fit the model.

check linear model assumption

We see that there the purple loess line is influnced by some potential outliers.
```{r}
# Partial regression plots are useful for identifying points with high leverage and influential data points that might not have high leverage
avPlots(lmod2.AIC)
# partial residual plot identify the type of relationship between y and each xi (given the effects of the other xj).
crPlots(lmod2.AIC)
par(mfrow = c(2,2))
plot(lmod2.AIC,which=1:4)
```
the constant variance assumption is not violated, and we didn't see a pattern for residuals, so linear assumption is not violated.
```{r}
#check the potential outliers
body[c(39,175,207,216,224,225),]
```
These observations seem acceptable. We do not have enough reason to remove these points.

###Probelm 3
![](12.2.png)
```{r}
x = rep(seq(1,50),2)
d = c(rep(0,50),rep(1,50))
y = 10 + x + d + 2*x*d + rnorm(100,0,10)
lmod3 = lm(y ~ x)
plot(lmod3$residuals ~ lmod3$fitted.values)
plot(y ~ x)
plot(lmod3,3)
```

Variance of the residual is not constant, as we see there is a pattern for residuals in this graph. The variance increases as the fitted values increase. We see that X is from [1,..., 50, 1,..., 50]. Y is 10+X for first 50 items and 10 + X + D + 2XD for the rest of 50 items. The regression line tries to minimize the errors, so as X increases, the residuals also increase. We can also see this from Y agains X plot.

###Problem 4
![](problem4.png)

<!-- \[\text{let } H_{(-j)} \text{ denote the projection matrix on all columns in X except } X_{(j)},\text{ the }j^{th} \text{ column} \\ -->
<!-- \text{then the residual in the regression of } Res(y, X^{-j}) \text{ is } Y^{(j)}=(I-H_{(-j)})y\\ -->
<!-- \text{the residual in the regression of } Res(X_j, X^{-j}) \text{ is } X^{(j)} = (I - H_{(-j)})X_j \\ -->
<!-- \text{ given the simple linear regression of }Y^{(j)} \text{ against } X^{(j)} \text{ has intercept 0 and slope } \hat{\beta_j} \\ -->
<!-- \text{ we can write the residual in this simple linear regression as }\\ -->
<!-- e^{(j)} = Y^{(j)} - \hat{\beta_j}X^{(j)}\\ -->
<!-- \text{we know }C(H_{(-j)}) \subseteq C(H)\\ -->
<!-- \text{ so } HH_{(-j)}=H_{(-j)} \\ -->
<!-- and\ H_{(-j)} \text{ is symmetric }, (HH_{(-j)})^T = H_{(-j)}^TH^T = H_{(-j)}H=(HH_{(-j)})\\ -->
<!-- \text{then }Y^{(j)} \text{ can be simplified, because }\\ -->
<!-- Y^{(j)} =(I-H_{(-j)})Y \\ -->
<!-- =Y-H_{(-j)}Y\\ -->
<!-- =Y - HH_{(-j)}Y \\ -->
<!-- =Y - H_{(-j)}HY\\ -->
<!-- =Y-H_{(-j)}\hat{Y}\\ -->
<!-- =Y - H_{(-j)}({\hat{\beta_0} + \hat{\beta_1}X_{1}} + \dots + \hat{\beta_p}X_{p} )\\ -->
<!-- =Y - ({\hat{\beta_0} + \hat{\beta_1}X_{1}} +  \dots + H_{(-j)}\hat{\beta_j}X_{j} +  \dots + \hat{\beta_p}X_{p}) \\ -->
<!-- =Y -( \hat{Y}-\hat{\beta_j}X_j + H_{(-j)}\hat{\beta_j}X_{j})\\ -->
<!-- =Y - \hat{Y} + \hat{\beta_j}X_j - H_{(-j)}\hat{\beta_j}X_{j}\\ -->
<!-- \text{ now we can express our residual in this simple linear regression as } \\ -->

<!-- e^{(j)} = Y^{(j)} - \hat{\beta_j}X^{(j)} = (I-H_{(-j)})Y - \hat{\beta_j}(I - H_{(-j)})X_j\\ -->
<!-- =Y - \hat{Y} + \hat{\beta_j}X_j - H_{(-j)}\hat{\beta_j}X_{j} - \hat{\beta_j}X_j + \hat{\beta_j}H_{(-j)}X_j\\ -->
<!-- =Y - \hat{Y} -->
<!-- =\hat{e}\] -->
![](p4.png)

###Problem 5 Determine if the statement below is true or false. (0.25 points each question)
a) Dropping a variable from the model will cause the other variables to become more significant.  
**FALSE, we can see results from backward elimination and observe that dropping a variable can either decrease or increase some other variables's p-value**

b) If an observation with high leverage is dropped from the model fitting, then the estimate for the coefficients will become more precise.  
**False, if a high leverage observation is not an influential observation, then the estimate for the coefficients doesn't change**

c) If AIC and Mallows Cp choose a best model and these both have the same number of parameters, then the models are identical (i.e., they include the same explanatory variables).  
$A I C ( m ) = n \log \left( \frac { R S S ( m ) } { n } \right) + n \log ( 2 \pi e ) + 2 ( 1 + p ( m ) ) )$
$C _ { p } ( m ) : = \frac { R S S ( m ) } { \hat { \sigma } ^ { 2 } } - ( n - 2 ( 1 + p ( m ) ) )$  
**True, comparing two formulas, we see that if p(m) are the same, then the only difference between two methods is a function of RSS. Then given the same number of parameters, both methods would choose the parameters that minimize RSS, so both method will choose sample explanatory variables**

d) R2 can be used as a model selection criterion in the linear model.  
**False, R^2 will always increase as the number of parameters increase. We use adjusted R-square instead**  

###Problem 6
Scientic Questions The principle scientic questions that this data pose are:
 Do different colonies use different foraging strategies? (e.e. worker-conservative versus energy-conservative) Is there some difference across size classes?
 Are there differences across colonies in the distribution of sizes or distances of the member ants?
```{r,echo=FALSE}
col1 = ant[ant$Colony == 1,]
col2 = ant[ant$Colony == 2,]
col3 = ant[ant$Colony == 3,]
col4 = ant[ant$Colony == 4,]
col5 = ant[ant$Colony == 5,]
col6 = ant[ant$Colony == 6,]
ant6 = rbind(col1,col2,col3,col4,col5,col6)
```

a) (0.5 points) First, examine the data visually, using various plots, including boxplots and coplots.

boxplots on Distance based on different Colony
```{r}
ggplot(data = ant6, aes(x= Colony, y=Distance)) +
 geom_boxplot(aes(group=Colony)) 
```

We see that colony 5 is different from other colonies. As we can see that colony 5 doesn't have workers whose distance is above 7.

Furthermore, we draw histogram on Distance for different colonies. We can see how many workers on different Distance for each colony.

```{r}
ggplot(data=ant6) + geom_histogram(aes(Distance),bins = 20)+facet_wrap(~Colony)
```

<!-- We can extract ants whose distance is 7 or 10 and see their relation between Distance and their Mass -->
we can see relation between Mass and Distance for each Colony
```{r,include=FALSE}

boxplot(Mass ~ Distance,xlab="Distance",ylab="Mass",data= col1)
boxplot(Mass ~ Distance,xlab="Distance",ylab="Mass",data= col2)
boxplot(Mass ~ Distance,xlab="Distance",ylab="Mass",data= col3)

boxplot(Mass ~ Distance,xlab="Distance",ylab="Mass",data= col4)
boxplot(Mass ~ Distance,xlab="Distance",ylab="Mass",data= col5)
boxplot(Mass ~ Distance,xlab="Distance",ylab="Mass",data= col6)
```

for all colones together
```{r}
boxplot(Mass ~ Distance,xlab="Distance",ylab="Mass",data= ant6)
```

```{r,echo=FALSE}
# how do use coplot?
# coplot(Mass ~ Distance  |Colony, data=ant6)
# coplot(Mass~Distance|factor(Colony), data = ant6)
# 
# coplot(Mass~Colony|factor(Distance), data = ant6)
# 
# coplot(Mass~Size.class|factor(Distance), data = ant6)

```

b) (2 points) Perform a regression of the mass on colony, distance, and size, and evaluate the appropriateness of your model using graphical techniques. If you find a transformation needed, justify your choice of transformation.

Because colony is a categorical variable, we turn it into a factor, just as size class is a factor.  
assume there's relation between colony and distance, we add an interaction term

<!-- If the skewness is between -0.5 and 0.5, the data are fairly symmetrical -->
<!-- If the skewness is between -1 and â€“ 0.5 or between 0.5 and 1, the data are moderately skewed -->
<!-- If the skewness is less than -1 or greater than 1, the data are highly skewed -->

<!-- take log(Mass) or not -->
```{r}
skewness(ant6$Mass)
#skewness of Mass is not big, so we don't neet to take the log

ant6$Colony = as.factor(ant6$Colony)
#assume there's relation between colony and distance, we add an interaction term
lmod6 = lm(Mass ~ Colony + Distance +Colony*Distance + Size.class, data=ant6)
summary(lmod6)

par(mfrow=c(2,2))
plot(lmod6,which=1:4)

ant6[ant6$Size.class == "\x80",]
#Only one data with size class = 80
```

We see the variance of residuals gets bigger as the fitted value increases. We didn't see a significant pattern for reisudals , so linear assumption seems ok. In qqplot, the normality assumption looks valid, although both tails are heavy. There are some potential outliers as we can see in cook's distance plot. We also notice there's one point that has very high leverage that needs to be investigated.

c) (1 points) Interpret the coeffcients relative to the scientic contributions and discuss what conclusions you can draw.


```{r,echo}
boxplot(Mass ~ Size.class,xlab="Size",ylab="Mass",data= ant6)
```

Across all colonies, we see a negative relationship between Mass and Distance. This means that the far the workers go, the less food they get. Overrall ants colony use energy conservative strategy.    
In summary table, colony and size class are dummy vairables. We can see differences among different colonies.  For example, colony 1 is the baseline. The mass of colony 2 on average is less than mass of colony 1 by 2.32 holding all other variables constant. Similar for size classes.  
From above boxplot and summary table, we see that on average size<30 has the lowest Mass. This means that in general, ants with the smallest size get the smallest amount of food on average. We also notice that ants whose size=80 don't get a lot of food. The size between 40-43 has the highest mean Mass, meaning the most food.  


