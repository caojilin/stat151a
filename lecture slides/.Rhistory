sort(influence(rgr1)$hat)
plot(sort(influence(rgr1)$hat), type = "h")
#The value of the leverage only depends on the explanatory values
npc = rbind(pc, c(100, 0))
plot(npc[,1], npc[,2])
rgr2 = lm(npc[,2] ~ npc[,1])
abline(rgr2)
abline(rgr, col = "red")
summary(rgr2)
influence(rgr2)$hat
mean(influence(rgr2)$hat)
sort(influence(rgr2)$hat)
plot(sort(influence(rgr2)$hat), type = "h")
npc = rbind(pc, c(100, -14))
plot(npc[,1], npc[,2])
rgr3 = lm(npc[,2] ~ npc[,1])
abline(rgr3)
summary(rgr3)
abline(rgr, col = "red")
influence(rgr3)$hat
mean(influence(rgr3)$hat)
sort(influence(rgr3)$hat)
#Points that are only outliers in y do not have high leverage
npc = rbind(pc, c(0, -14))
plot(npc[,1], npc[,2])
rgr4 = lm(npc[,2] ~ npc[,1])
abline(rgr4)
abline(rgr, col = "red")
influence(rgr4)$hat
mean(influence(rgr4)$hat)
sort(influence(rgr4)$hat)
plot(sort(influence(rgr4)$hat), type = "h")
summary(rgr4)
summary(rgr)
#Now the estimated regression line is not all that different. But the R^2 value is very different.
#Now the estimated regression line is not all that different. But the R^2 value is very different.
#Now the estimated regression line is not all that different. But the R^2 value is very different.
#Now the estimated regression line is not all that different. But the R^2 value is very different.
#Now the estimated regression line is not all that different. But the R^2 value is very different.
#Now the estimated regression line is not all that different. But the R^2 value is very different.
#Now the estimated regression line is not all that different. But the R^2 value is very different.
#Now the estimated regression line is not all that different. But the R^2 value is very different.
rgr4 = lm(npc[,2] ~ npc[,1])
abline(rgr4)
abline(rgr, col = "red")
influence(rgr4)$hat
mean(influence(rgr4)$hat)
sort(influence(rgr4)$hat)
plot(sort(influence(rgr4)$hat), type = "h")
summary(rgr4)
summary(rgr)
data()
HairEyeColor
Orange
plot(age ~ circumference, data=Orange)
Titanic
str(Titanic)
summary(Titanic)
Titanic
head(Titanic)
colnames(Titanic)
strTitanic
str(Titanic)
Titanic$Class
Titanic$
fsaf
class(Titanic)
typeof(Titanic)
Titanic[1]
Titanic[10]
data(Titanic)
a = data(Titanic)
a
a = data(list=Titanic)
a
Titanic[120]
Titanic[120]
length(Titanic)
Titanic[1]
b=Titanic[1]
b$Age
table(women)
table(mtcars)
table(HairEyeColor)
HairEyeColor''
HairEyeColor
HairEyeColor$Female
HairEyeColor[2]
HairEyeColor[1]
length(HairEyeColor)
HairEyeColor[1]
HairEyeColor[5]
HairEyeColor[20]
waders
synth.te
volcano
sleep
data(package = .packages(all.available = TRUE))
TitanicSurvival
carData::carData
carData::TitanicSurvival
aa =carData::TitanicSurvival
str(aa)
View(aa)
filter(aa,survived=yes)
aa[survived=yes]
aa[survived,]
aa$survived
aa[aa$survived="yes"]
aa[aa$survived="yes",]
aa[aa$survived="yes",]
aa[while (sex="female") {
}]
aa[which(sex="female")]
aa[which(sex="female"),]
aa[which(sex='female'),]
aa
women[height > 100]
women[height > 100,]
women[women$height > 100,]
women[women$height > 100]
women[,women$height > 100]
women[,women$height > 100,]
women[\women$height > 100,]
women[women$height > 100,]
women[women$height > 100,]
women[women$height > 10,]
women[height > 10,]
aa[aa$survived]
aa$survived[aa$sex='female']
aa$survived[aa$sex='female',]
train = read.csv("./train.csv",stringsAsFactors = FALSE)
test = read.csv("./test.csv",stringsAsFactors = FALSE)
str(train)
table(train$Survived)
prop.table(table(train$Survived))
test$Survived <- rep(0, 418)
submit <- data.frame(PassengerId = test$PassengerId, Survived = test$Survived)
write.csv(submit, file = "theyallperish.csv", row.names = FALSE)
str(train)
table(train$Survived)
prop.table(table(train$Survived))
train = read.csv("./train.csv")
summary(train$Sex)
table(train$Sex,train$Survived)
prop.table(table(train$Sex,train$Survived),1)
aa
aa$survived[aa$sex='male']
aa[aa$sex='male']
aa[aa$sex='male',]
aa[aa$sex=='male']
aa[aa$sex=='male',]
aa$survived[aa$sex=='male',]
aa$survived[aa$sex=='male']
length(aa$survived[aa$sex=='male'])
length(aa$survived[aa$sex=='male'])/nrow(aa)
data(Duncan)
library(car)
data(Duncan)
Duncan
knitr::opts_chunk$set(
cache=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE,
results = 'markup', dev='png', dpi=150
)
library(car)
data(Duncan)
head(Duncan)
mod <- lm(prestige ~ income + education, data=Duncan)
X <- cbind(1, Duncan[,c("income", "education")])
X <- unname(as.matrix(X))
# compute the hat values
H <- X %*% solve(t(X) %*% X) %*% t(X)
diag(H)
lev <- diag(H)
n <- dim(Duncan)[1]
p <- 2
plot(1:n, lev)
# get the values with high leverage
lev.sorted <- sort(lev, decreasing=T, index.return=T)
lev.sorted$x[1:3]
rownames(Duncan)[lev.sorted$ix[1:3]]
plot(1:n, lev)
for(i in lev.sorted$ix[1:3]) {
text(i, lev[i]-0.02, rownames(Duncan)[i])
}
hbar <- (p+1)/n
abline(h=2*hbar, lty=2)
abline(h=3*hbar, lty=2)
plot(Duncan$education, Duncan$income, xlim=c(-20, 120), ylim=c(-40, 120), xlab="education", ylab="income")
i = lev.sorted$ix[1]
text(Duncan$education[i]-20, Duncan$income[i], rownames(Duncan)[i], cex=0.8)
i = lev.sorted$ix[2]
text(Duncan$education[i]-20, Duncan$income[i], rownames(Duncan)[i], cex=0.8)
i = lev.sorted$ix[3]
text(Duncan$education[i]+20, Duncan$income[i], rownames(Duncan)[i], cex=0.8)
centroid <- c(mean(Duncan$education), mean(Duncan$income))
points(centroid[1], centroid[2], pch=15)
Xstar <- scale(X[,2:3], scale=F) # centering columns
A <- (t(Xstar) %*% Xstar)
ellipse(centroid, shape=A, radius=sqrt(2*hbar - 1/n), col=1, lty=2, center.cex=0)
ellipse(centroid, shape=A, radius=sqrt(3*hbar - 1/n), col=1, lty=2, center.cex=0)
y <- Duncan$prestige
# predicted values;
y_hat <- H %*% y
e_hat <- y_hat - y
plot(e_hat, title = 'estimated residuals')
e_hat_order_indx <- order(abs(e_hat), decreasing = TRUE)
i <- e_hat_order_indx[1]
text(i, e_hat[i] + 4, rownames(Duncan)[i])
i <- e_hat_order_indx[2]
text(i, e_hat[i] - 4, rownames(Duncan)[i])
i <- e_hat_order_indx[3]
text(i, e_hat[i] - 4, rownames(Duncan)[i])
# standardized residuals
sigma_hat <- sqrt(sum(e_hat**2) / (n - p - 1))
e_hat_std <- e_hat / (sigma_hat * sqrt(1 - diag(H)))
plot(e_hat_std, ylab = 'standardized residual')
e_hat_std_order_indx <- order(abs(e_hat_std), decreasing = TRUE)
i <- e_hat_std_order_indx[1]
text(i, e_hat_std[i] + 0.4, rownames(Duncan)[i])
i <- e_hat_std_order_indx[2]
text(i, e_hat_std[i] - 0.4, rownames(Duncan)[i])
i <- e_hat_std_order_indx[3]
text(i, e_hat_std[i] - 0.4, rownames(Duncan)[i])
# Predicted residuals
e_hat_pred <- e_hat / (1 - diag(H))
plot(e_hat_pred, ylab = 'predicted residual')
e_hat_pred_order_indx <- order(abs(e_hat_pred), decreasing = TRUE)
i <- e_hat_pred_order_indx[1]
text(i, e_hat_pred[i] + 5, rownames(Duncan)[i])
i <- e_hat_pred_order_indx[2]
text(i, e_hat_pred[i] - 5, rownames(Duncan)[i])
i <- e_hat_pred_order_indx[3]
text(i, e_hat_pred[i] - 5, rownames(Duncan)[i])
# get the values with high leverage
lev.sorted <- sort(lev, decreasing=T, index.return=T)
lev.sorted$ix
lev.sorted$
x
?abline
lev.sorted$ix
?scale
require(stats)
x <- matrix(1:10, ncol = 2)
x
(centered.x <- scale(x, scale = FALSE))
(centered.x <- scale(x, scale = TRUE))
40/5
55/10
cov(centered.scaled.x <- scale(x)) # all 1
sd(x[,2])
Duncan
View(Duncan)
s = sd(centered.x[,2])
s
centered.x
a=scale(x)
a
a=scale(x,scale = FALSE)
A
A
X
x <- matrix(1:10, ncol = 2)
a=scale(x)
a
x <- matrix(1:10, ncol = 2)
a=scale(x,scale = FALSE)
a
a=scale(x,scale = 1)
a=scale(x,scale = c(1,1,))
a=scale(x,scale = c(1,1))
a
a=scale(x,scale = c(2,2))
a
a=scale(x,scale = FALSE)
a
s = sd(a[,2])
s
x
centered.x
a
a/s
?text
plot(-1:1, -1:1, type = "n", xlab = "Re", ylab = "Im")
K <- 16; text(exp(1i * 2 * pi * (1:K) / K), col = 2)
exp(1i * 2 * pi * (1:K) / K)
plot(1:10, 1:10, main = "text(...) examples\n~~~~~~~~~~~~~~",
sub = "R is GNU ©, but not ® ...")
points(c(6,2), c(2,1), pch = 3, cex = 4, col = "red")
text(6, 2, "the text is CENTERED around (x,y) = (6,2) by default",
cex = .8)
text(4, 9, expression(hat(beta) == (X^t * X)^{-1} * X^t * y))
e_hat_std_order_indx <- order(abs(e_hat_std), decreasing = TRUE)
e_hat_std_order_indx
i <- e_hat_std_order_indx[1]
i
text(i, e_hat_std[i] + 0.4, rownames(Duncan)[i])
plot(e_hat_std, ylab = 'standardized residual')
e_hat_std_order_indx <- order(abs(e_hat_std), decreasing = TRUE)
i <- e_hat_std_order_indx[1]
text(i, e_hat_std[i] + 0.4, rownames(Duncan)[i])
i
e_hat_std[i]
i
rownames(Duncan)[i]
#Lecture Six (Regression terminology, fitted values, residuals etc.)
#When X^T X is not invertible.
#Consider the body fat dataset
body = read.delim("bodyfat_corrected.txt", header = TRUE, sep = "")
body=read.csv("BodyFat.csv")
head(body, 10)
#Let us fit a linear model for BODYFAT using the explanatory variables AGE, HEIGHT, WEIGHT and THIGH.
lm1 = lm(BODYFAT ~ AGE + WEIGHT + HEIGHT + THIGH, data = body)
View(body)
#Let us fit a linear model for BODYFAT using the explanatory variables AGE, HEIGHT, WEIGHT and THIGH.
# lm1 = lm(BODYFAT ~ AGE + WEIGHT + HEIGHT + THIGH, data = body)
lm1 = lm(bodyfat ~ Age + Weight + Height + Thigh, data = body)
summary(lm1)
xmat = matrix(0, nrow(body), 5)
xmat[,1] = rep(1, nrow(body))
xmat[,2] = body$AGE
xmat[,3] = body$WEIGHT
xmat[,4] = body$HEIGHT
#Let us fit a linear model for BODYFAT using the explanatory variables AGE, HEIGHT, WEIGHT and THIGH.
# lm1 = lm(BODYFAT ~ AGE + WEIGHT + HEIGHT + THIGH, data = body)
lm1 = lm(bodyfat ~ Age + Weight + Height + Thigh, data = body)
summary(lm1)
xmat = matrix(0, nrow(body), 5)
xmat[,1] = rep(1, nrow(body))
xmat[,2] = body$Age
xmat[,3] = body$Weight
xmat[,4] = body$Height
xmat[,5] = body$Thigh
H = xmat %*% (solve(t(xmat) %*% xmat))%*%t(xmat)
yvec = body$bodyfat
H
H[2]
H[,2]
sum(H[,2]^2)
H[2]
H[0]
H[1]
H[2:]
H[2:length(H)]
H
H[1,1]
a = H[1,1]
b = H[1,]
a
head(b)
b[-1]
b
sum(b[-1]^2)
a
a-a^@
a-a^2
acf()
?acf
lm1
acf(lm1$residuals)
qqnorm(lm1$residuals)
ppoints(lm1$residuals)
a = ppoints(lm1$residuals)
pnorm(a)
plot(pnorm(a))
plot(sort(lm1$residuals) ~ a)
qqnorm(lm1$residuals)
plot(sort(lm1$residuals) ~ a)
plot(sort(lm1$residuals) ~ pnorm(a))
plot(sort(lm1$residuals) ~ pnorm(a))
qqnorm(lm1$residuals)
#The scatter plots involving the variables distance and NoOfPools look a little odd. It makes sense to consider transforming them.
summary(frogs$distance)
library(DAAG)
data(frogs)
help(frogs)
plot(northing ~ easting, data=frogs, pch=c(1,16)[frogs$pres.abs+1],
xlab="Meters east of reference point", ylab="Meters north")
#The scatter plots involving the variables distance and NoOfPools look a little odd. It makes sense to consider transforming them.
summary(frogs$distance)
plot(density(frogs$distance))
plot(density(log(frogs$distance)))
summary(frogs$NoOfPools)
plot(density(frogs$NoOfPools))
plot(density(log(frogs$NoOfPools)))
#Null deviance is the deviance (-2*max log likelihood) in a model that only has the intercept term
#If the model only has the intercept term, then p_i is the same for each i and it is therefore fitted by the value:
ybar = mean(frogs$pres.abs)
ybar
# For probit:  binomial(link = "probit")
# Default link function for binomial is logit
summary(frogs.glm0)
#Fit the logistic model:
frogs.glm0 <- glm(formula = pres.abs ~ altitude + log(distance) +
log(NoOfPools) + NoOfSites + avrain + meanmin + meanmax,
family = binomial, data = frogs)
# For probit:  binomial(link = "probit")
# Default link function for binomial is logit
summary(frogs.glm0)
nrow(frogs)
212-7
a <- glm(formula = pres.abs ~ log(distance) +
log(NoOfPools) + meanmin,
family = binomial, data = frogs)
summary(a)
nrow    a
nrow(a)
nrowa
a
# For probit:  binomial(link = "probit")
# Default link function for binomial is logit
summary(frogs.glm0)
summary(a)
4.11947+0.30228*log(157) +0.32586*log(0.868+0.001) + 0.40984*log(2.894+0.001)
log(10)
e^(6.037771)
e
log(1)
e
2.718281828^6.037771
418.9581/(1+418.9581)
pairs(dd)
rm(list = ls())
#Ozone dataset from the library Faraway:
library(faraway)
data(ozone, package = "faraway")
help(ozone)
dd = ozone
head(ozone)
pairs(dd)
str(dd)
#O3 is the response variable. The rest are explanatory variables.
#Recursive Partitioning algorithm
library(rpart)
rt = rpart(O3 ~ ., data = dd)
plot(rt, margin = 0.1)
text(rt)
#understanding the output
rt
#If the clause at the beginning of a node is met, we go left and if it is not met, we go left.
#The depth of the branches is proportional to the reduction in error due to the split. The TSS here is 21115.4100:
sum((dd$O3 - mean(dd$O3))^2)
#If you want to plot the tree with uniform spacing, use
plot(rt, compress = T, uniform = T, branch = 0.4, margin = 0.1)
text(rt)
#The tree generated by rt has been constructed from a large tree by cost-complexity pruning
printcp(rt)
#This gives the best cross-validated error for various values of cp = alpha. The term relative error is simply RSS(T)/TSS. This will therefore always decrease as the tree complexity increases. The xerror is an error calculated by 10-fold cross-validation (and divided by TSS). Because the partition of data into 10 parts is random, xerror is random and xstd provides its standard deviation.
#The tree generated by rpart corresponds to the cp for which xerror is the smallest. If we think that the tree generated by rpart is too big, we can prune it by selecting our own value of cp:
#Using xerror and xstd, we can choose a higher value of cp for a more interpretable tree if we so desire.
rta = prune.rpart(rt, 0.026756)
quartz()
plot(rta, margin = 0.1)
text(rta)
#Using a smaller value of cp:
rt = rpart(O3 ~ ., data = dd, cp = 0.001)
plot(rt, margin = 0.1)
text(rt)
printcp(rt)
rta = prune.rpart(rt, 0.02315)
quartz()
plot(rta, margin = 0.1)
text(rta)
#"R^2" for regression tree:
1 - (sum(residuals(rta)^2))/(sum((ozone$O3 - mean(ozone$O3))^2))
#Ozone dataset from the library Faraway:
library(faraway)
data(ozone, package = "faraway")
help(ozone)
dd = ozone
head(ozone)
pairs(dd)
#O3 is the response variable. The rest are explanatory variables.
#Recursive Partitioning algorithm
library(rpart)
rt = rpart(O3 ~ ., data = dd)
plot(rt, margin = 0.1)
text(rt)
#understanding the output
rt
knitr::opts_chunk$set(echo = TRUE, fig.show="hold", cache=T)
library(ggplot2)
library(ggplot2)
library(reshape2)
library(DAAG)
?DAAG
data(spam7)
help(spam7)
spam = spam7
library(rpart)
sprt = rpart(yesno ~ ., method = "class", data = spam)
plot(sprt, margin=0.1)
text(sprt)
plot(sprt, margin=0.1)
text(sprt)
sprt
plot(sprt, margin=0.1)
text(sprt)
sprt
lot(sprt, margin=0.1)
margin=0.1)
plot(sprt, margin=0.1)
text(sprt)
plot(isprt, margin=0.1)
plot(isprt, margin=0.1)
plot(sprt, margin=0.1)
text(sprt)
sprt
deviance(sprt)
1813/4601
816/3471
246/2420
