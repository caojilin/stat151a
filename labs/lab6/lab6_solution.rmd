---
title: "Stat 151A Lab 6"
author: "Bryan Liu"
date: "September 28, 2018"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
urlcolor: blue
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE, 
  results = 'markup', dev='png', dpi=150
)

library(tidyverse)
```

# Confidence and Prediction intervals

We run up a simulation to assert the correctness of our confidence intervals under the linear regression model. 

Let us set up the data generating process, 
\begin{align}
y \sim \mathcal{N}(X\beta, \sigma^2 I_{d\times d})
\end{align}
```{r}
#################
# draw data according to the linear regression model. 
#################
n_obs <- 100 # number of observations 
sig <- 10 # std error of the y's

x <- runif(n_obs, 0, 10) # pick x's: from this point on, x will be FIXED

# true coefficients
beta0 <- 32
beta1 <- 7

draw_data <- function(x, beta0, beta1, sig){
  # draw y's from the fixed x's and the true regression coefficients

  n_obs <- length(x)
  
  y = rnorm(n_obs, mean = beta0 + beta1*x, sd = sig) 
  
  return(data.frame(x = x, y = y))
}

# get the dataset, and plot
reg_data <- draw_data(x, beta0, beta1, sig)

ggplot(reg_data) + 
  geom_point(aes(x, y)) + 
  geom_smooth(aes(x, y), method = 'lm', level = 0)

```

For whatever reason, we are especially interested in guessing what happens at $x_0 = 8$. There are two ways to think about this. We might be particularly interested in the *mean* of $y_0$ at $x_0 = 8$, given by $E(y | x = 8) = \beta_0 + 8\beta_1$. Or we might be interested in guessing what a new $y_0$ drawn from $\mathcal{N}(\beta_0 + 8\beta_1, \sigma^2)$. 

In any case, we want to have a sense of how good our estimate is. To do so, we construct a confidence interval. 

Let us digress for a moment and talk about confidence intervals in general. Suppose our quantitity of interest was some unknown parameter $\theta^*$; then a 95\% confidence interval $\mathcal{I}$ is a subset of $\mathbb{R}$ that statisfies 
\begin{align}
P_{\theta^*}(\theta^* \in \mathcal{I}) = 0.95
\end{align}
What is random in this probability statement?

Returning the regression case, we wish to construct an interval $\mathcal{I}_c$ such that 
\begin{align}
P(E(y | x = 8) \in \mathcal{I}_c) = 0.95
\end{align}
Such an $\mathcal{I}_c$ is what we call a *confidence* interval in the regression context. What is random in this probability statement?

Or in the second case, we may wish to construct an $\mathcal{I}_p$ such that 
\begin{align}
P(y_0 \in \mathcal{I}_p) = 0.95 
\end{align}
Such an $\mathcal{I}_p$ is what we call a *prediction* interval in the regression context. What is random in this probability statement?

In class, we showed that 
\begin{align}
\mathcal{I}_c &= \hat y_0 + \hat\sigma [x_0^T (X^TX)^{-1}x_0] t^{1 - \alpha / 2}_{n - 2}\\
\mathcal{I}_p &= \hat y_0 + \hat\sigma [1 + x_0^T (X^TX)^{-1}x_0] t^{1 - \alpha / 2}_{n - 2}
\end{align}
satisfiy the defintions of a confidence interval described above. $t^{1 - \alpha / 2}_{n - 2}$ is the $(1 - \alpha/2)$th quantile of a $t$-distribution with $n - 2$ degrees of freedom. Shall we go through the derivations again here?

We check the validty of both these intervals with simulations. 

```{r}
get_interval <- function(reg_data, x0, alpha = 0.05, prediction = FALSE){
  # function that returns the estimate, and 
  # the upper and lower confidence bounds 
  
  n_obs <- dim(reg_data)[1]
  
  # X matrix for this simple linear regression problem 
  x_mat <- matrix(c(rep(1, n_obs), reg_data$x), nrow = n_obs)
  
  # X^T X 
  xtx <- t(x_mat) %*% x_mat
  
  # X^T y
  xty <- t(x_mat) %*% reg_data$y
  
  # regression coefficients
  beta <- solve(xtx, xty)
  
  # prediction at x0
  y0 <- beta[1] + beta[2] * x0
  
  # estimate of data variance
  y_hat <- beta[1] + beta[2] * reg_data$x
  sigma_hat <- sqrt(1 / (n_obs - 2) * sum((reg_data$y - y_hat)**2))
  
  # estimate of coeff. variance
  x0_vec <- matrix(c(1, x0), nrow = 2)
  
  # whether or not we are doing prediction interval
  pred_adj <- 0
  if(prediction){
    pred_adj <- 1}

  coef_var <- sqrt(pred_adj + t(x0_vec) %*% solve(xtx, x0_vec))
  
  # t statistic
  t <- qt(1 - alpha / 2, n_obs - 2)
  
  # upper and lower bounds
  ub <- y0 + t * sigma_hat * coef_var
  lb <- y0 - t * sigma_hat * coef_var
  return(list(y0 = y0, 
              lb = lb, 
              ub = ub))
}
```
1 covering the avg height at 140 pounds.
2 new person y0, try to get a 95% CI for his height. yo is drew from a normal(X0Tbeta, sigma^2). More randomness here, so CI is wider.


Let us assert the correctness of the conficence bound. 
```{r}
# we predict at x0 = 8
x0 <- 8.0

# The true value expected value for y0 is 
true_e_y0 <- beta0 + beta1 * x0

# draw data, see how many times the interval covers true_e_y0
n_trials <- 2000
covered_bool <- rep(FALSE, n_trials)
for(n in 1:n_trials){
  reg_data <- draw_data(x, beta0, beta1, sig)
  pred_interval <- get_interval(reg_data, x0, alpha = 0.05, prediction = FALSE)
  
  covered_bool[n] <- (true_e_y0 < pred_interval$ub) &
                      (true_e_y0 > pred_interval$lb)
}

# how many times did we cover the true interval?
mean(covered_bool)
```

Our confidence bound is too small if we are trying to cover new draws of y0: 
```{r}
# draw data, see how many times the interval covers true_e_y0
n_trials <- 2000
covered_bool <- rep(FALSE, n_trials)
for(n in 1:n_trials){
  reg_data <- draw_data(x, beta0, beta1, sig)
  
  # draw new y0. 
  # This is key: before, only the dataset was random. 
  y0_new <- rnorm(1, mean = beta0 + beta1 * x0, sd = sig)
  
  # NOTE2: prediction is FALSE here
  # we are going to undercover 
  pred_interval <- get_interval(reg_data, x0, alpha = 0.05, prediction = FALSE)
  
  covered_bool[n] <- (y0_new < pred_interval$ub) &
                      (y0_new > pred_interval$lb)
}


# how many times did we cover the true interval?
mean(covered_bool)
```

Thus, we have the inflate our interval. This is known as a prediction interval: 
```{r}
# draw data, see how many times the interval covers a new draw y0
n_trials <- 2000
covered_bool <- rep(FALSE, n_trials)
for(n in 1:n_trials){
  reg_data <- draw_data(x, beta0, beta1, sig)
  
  # draw new y0. 
  # This is key: before, only the dataset was random. 
  # here we are drawing a new y
  y0_new <- rnorm(1, mean = beta0 + beta1 * x0, sd = sig)
  
  # NOTE2: prediction is TRUE here
  pred_interval <- get_interval(reg_data, x0, alpha = 0.05, prediction = TRUE)
  
  # see if we caught the new y value
  covered_bool[n] <- (y0_new < pred_interval$ub) &
                      (y0_new > pred_interval$lb)
}


# how many times did we cover the true interval?
mean(covered_bool)
```
So we are OK now. 

# Permutation tests

## The dataset
We consider a dataset from Andrew Gelman's book, *Bayesian data analysis.* (Don't worry, the analysis we do here will not be Bayesian). 

The data is on 50 cows, and an experiment is run to test the effect of four different levels of a dietary supplements. The outcome of interest for us will be the final weight of the cow, after several weeks of taking the dietary supplement. 

We regress the final weight of the cow on its age, its initial weight, and the level of dietary supplement the cow was given. 

```{r}
# load the data
cow_data <- read.csv('./cow_data.txt', sep = '')
  
# fit a multiple linear regression of final weight on level (the treatment)
# and two other covariates: age and initial weight
model<- lm(final.weight ~ level + age + initial.weight, data = cow_data)
summary(model)

```
No surprise that initial weight is a very strong predictor of final weight. But is the coefficient for ``level``, i.e. our treatment, significant? It is displayed here:
```{r}
beta_treat_og <- model$coefficients['level']
beta_treat_og
```

And a quick residual analysis
```{r}
residuals <- model$residuals
predicted <- model$fitted.values

ggplot() + geom_point(aes(x = predicted, y = residuals)) + 
  geom_hline(yintercept = 0.0, color = 'red')

ggplot() + geom_histogram(aes(x = residuals), bins = 15, 
                          color = 'blue', fill = 'light blue')
```

### Permutation tests
We want to test if there is a significant effect of the dietary supplement on the cow's future weight. Let us assume proper treatment randomization amongst the cows. This then allows us to draw causal claims. But how to test for causal claims, and why are causal conclusions valid under randomization?

NOTE: randomization does NOT JUSTIFY THE NORMALITY ASSUMPTIONS! And in this case, there isn't enough data to argue for CLT. Hence, the significance t-statistics may not be valid. 

So what can we do? 

Note that if we assume proper randomization, the permutation test IS valid under the the null hypothesis of no treatment effect. Why? Clearly state the null hypothsis and the assumptions needed. 


We run the permutation test: 
```{r}
permute_treatment_get_coeff <- function(cow_data){
  # function to permute treatment variable
  # and recompute regression coefficients
  
  n_obs <- dim(cow_data)[1]
  
  # get random permutation of the indices
  perm_indices <- sample(n_obs, n_obs, replace = FALSE)
  
  # permute the cow data: but ONLY THE TREATMENT VARIABLE
  cow_data_perm <- cow_data
  cow_data_perm$level <- cow_data$level[perm_indices]
  
  # recompute the regression coefficient for the treatment variable 
  model_perm <- lm(final.weight ~ level + age + initial.weight, 
                   data = cow_data_perm)
  treatment_coeff_perm <- model_perm$coefficients['level']
  
  return(treatment_coeff_perm)
}

# run the permutation test
n_perm <- 2000
beta_treat_vec <- rep(0, n_perm)
for(i in 1:n_perm){
  beta_treat_vec[i] <- permute_treatment_get_coeff(cow_data)
}

# plot the histogram of the treatment coefficient under 
# the permutation distribution 
# red line was our originally computed statistic
ggplot() + geom_histogram(aes(x = beta_treat_vec), 
                          color = 'blue', fill = 'light blue') + 
  geom_vline(xintercept = beta_treat_og, color = 'red')
```
The histogram shows the distribution of the treatment coefficient ($\beta_1$) 
under the permutation distribution. The red line was the $\beta_1$ computed from the original dataset. What do we conclude?

# The bootstrap

### The dataset: 

The example we consider is from Brad Efron and Trevor Hastie's book, *Computer Age Statistical Inference*. The data measures the performace of 22 students on five different math/physics tests. 

```{r}
student_scores <- read.csv('./student_score.txt', sep = ' ')
head(student_scores)
pairs(student_scores)
```

Let us compute the correlation matrix: 
```{r}
student_score_corr <- cor(student_scores)
```

And suppose we were interested the ``eigenratio'' statistic of the correlation matrix, given by 
\begin{align}
\hat\theta = \text{largest eigenvalue} / \text{sum eigenvalues}
\end{align}
Loosely, this measures how well each of the five scores can be predicted by a just a single linear vector. We might think of this linear vector as an "IQ" score of some sort, and we are asking how strongly this IQ score predicts the results of these tests. 

In any case, the estimate we get is: 
```{r}
get_eigen_ratio <- function(student_scores){
  # function that returns the eigenratio statistic from a dataset
  
  # get correlation
  student_score_corr <- cor(student_scores)
  
  # get eigenvalues
  e_vals <- eigen(student_score_corr, symmetric = TRUE)$values
  
  # return eigenration
  return(max(e_vals) / sum(e_vals))
}

eigen_ratio <- get_eigen_ratio(student_scores)
eigen_ratio
```
How accurate is this estimate $\hat\theta$? In particular, what is its standard error?

This statistic is a very complicated, nonlinear function of our observations. It is not clear the CLT applies. So how can we get confidence bounds?

### The bootstrap
The bootstrap provides an alternative to estimate standard errors and confidence intervals.  

```{r}
# some functions
sample_data <- function(student_scores){
  # function to create a bootstra dataset
  # from the original 
  
  # number of observations in the dataset
  n_obs <- dim(student_scores)[1]
  
  # find indices to sample
  boot_sample_indices <- sample(n_obs, n_obs, replace = TRUE)
  student_scores_boot <- student_scores[boot_sample_indices, ]
  
  return(student_scores_boot)
}

run_bootstrap <- function(student_scores, n_boot = 2000){
  # returns a vector of the eigenratios obtained from 
  # bootstrap sampling the data 
  
  ev_ratio_boot_vec <- rep(0, n_boot)
  for(b in 1:n_boot){
    student_scores_boot <- sample_data(student_scores)
    
    ev_ratio_boot_vec[b] <- get_eigen_ratio(student_scores_boot)
  }
  return(ev_ratio_boot_vec)
}
```

```{r}
# We run the bootstrap
ev_ratio_boot_vec <- run_bootstrap(student_scores, n_boot = 2000)

ggplot() + geom_histogram(aes(x = ev_ratio_boot_vec), 
                          color = 'blue', 
                          fill = 'light blue') + 
  xlab('eigen ratio') + 
  geom_vline(xintercept = eigen_ratio, color = 'red')
```

We can estimate the standard errors from the bootstrap distribution: 
```{r}
sd(ev_ratio_boot_vec)
```

And we can use the quantiles of the bootstrapped distribution to get confidence intervals: 
```{r}
# upper confidence bound: 
quantile(ev_ratio_boot_vec, 0.975)

# lower confidence bound: 
quantile(ev_ratio_boot_vec, 0.025)
```

So why is the bootstrap a good idea? Suppose our data $x_1, ..., x_n$ were drawn i.i.d from some distribution $F$. In our case, each $x_i$ is a five dimensional vector with the five test scores of student $i$. We compute our statistic $\hat\theta$, the eigenratio statistic for example, and it is some complicated function of $x_1, ..., x_n$. Note that since it is a function of data, $\hat\theta$ also has some distribution under $F$. 

Concretely, suppose we are interested in the variance of $\hat\theta$ (but really, we could ask for any property of the distribution of $\hat\theta$). To compute this variance, we could, theoretically, repeatedly draw data $x_1, ..., x_n$ from the true distribution $F$, each time computing a new $\hat\theta$. We then estimate the variance from these samples. 

However, in reality, we do not know what $F$ is. Hence, we approximate $F$ by $F_n$, the *empirical distribution*, i.e. the distribution that puts weight $1/n$ on each datapoint $x_i$. We then sample new datasets from $F_n$, each time computing a new $\hat\theta$. We estimate the variance from these samples. This is exactly the bootstrap procedure. 