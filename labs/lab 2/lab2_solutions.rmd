---
title: "Stat 151A Lab 2"
author: "Bryan Liu"
date: "August 31, 2018"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
urlcolor: blue
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE, 
  results = 'markup', dev='png', dpi=150
)

#####
# settings for my pretty plots (bc Bryan is picky)
require('tidyverse')

blank_theme <- theme_minimal(base_size = 30) +
  theme(plot.background = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        panel.border = element_blank()) + 
  theme(axis.line.x = 
          element_line(colour = 'black', 
                      size=0.5, linetype='solid')) +
  theme(axis.line.y = element_line(colour = 'black',
                      size=0.5, linetype='solid'))

```

# The wages dataset
We consider cross-sectional wage data sampled from the 1976 US Population Survey[^1]. It consists of 526 observations of average hourly wages, and 
various covariates such as education, race, gender, or marital status. 

[^1]: Wooldridge, J.~M. (2000), Introductory Econometrics: A Modern Approach, South-Western College Publishing.

### Loading the dataset
```{r}
# uncomment the install.packages if you do not have this package
# install.packages('wooldridge')
require('wooldridge')
data('wage1')
head(wage1)

```

Wage is in dollars per hour. Lets look at its distribution: 
```{r}
# A histogram of wages
wage1 %>% ggplot() + 
  geom_histogram(aes(x = wage), 
                  fill = 'light blue', 
                  color = 'blue') + blank_theme 

# For demonstrating ggplot: 
# we can color by gender, for example
# wage1 %>% mutate(gender = as.factor(female)) %>%
  # ggplot() + 
  # geom_histogram(aes(x = wage, color = gender, fill = gender, dodge = TRUE)) +
  # blank_theme 
```


### Simple linear regression
We are going to consider the relationship between education and wages. 
Lets make a scatterplot! 
```{r}
# Make a scatter plot with wages on the y axis, and education on the x axis
wage1 %>% 
  ggplot() + 
  geom_point(aes(x = educ, y = wage), color = 'blue') + 
  blank_theme + 
  geom_smooth(aes(x = educ, y = wage), method = "lm", 
                        col = "red", se = FALSE)
```


Print the coefficients of the linear model regressing wage on education: 
```{r}
slm <- lm(wage ~ educ, data = wage1)
summary(slm)

# abline doesn't work in knitr for some reason
# but works in the console
# y = wage1$wage
# x = wage1$educ
# plot(x, y, xlab = "Education", ylab = "Hourly Wages")
# abline(slm, col = "red")
```
What is the estimated intercept term? What does it mean? Does it make sense?

What is the estimated slope? What does it mean?

Its usually a good ideas to look at residuals. Compute and plot the 
residuals here: 
```{r}
# Compute residuals
pred_wage <- predict(slm, newdata = wage1)
residuals <- pred_wage - wage1$wage

# Plot residuals against education
ggplot() + geom_point(aes(x = wage1$educ, y = residuals)) + 
  geom_hline(yintercept = 0.0, color = 'red') +
  xlab('education') + ylab('residuals') + blank_theme
```
Observations about residuals?

Lets redo the plot, but use a log scale for wages. 
Why might the log scale make more sense?
```{r}
# Make a scatterplot of log(wage) vs education
wage1 %>% mutate(log_wage = log(wage)) %>% 
  ggplot() + geom_point(aes(x = educ, y = log_wage)) + blank_theme + 
            geom_smooth(aes(x = educ, y = log_wage), method = "lm", 
                        col = "red", se = FALSE)

```
Examine the regression coefficients for log wage vs educ: 
```{r}
# Print the regression coefficients for log(wage) vs education
slm2 <- lm(log(wage) ~ educ, data = wage1)
summary(slm2)
```

Compute residuals again: 
```{r}
# Compute residuals for log(wage) vs education
pred_logwage <- predict(slm2, newdata = wage1)
residuals2 <- pred_logwage - log(wage1$wage)

# Plot residuals against education
ggplot() + geom_point(aes(x = wage1$educ, y = residuals2)) + 
  geom_hline(yintercept = 0.0, color = 'red') +
  xlab('education') + ylab('residuals') + blank_theme
```
Better?

# A simulation experiment
There is an unknown line: 
```{r}
# True parameters (unknown)
beta0 = 32
beta1 = 0.5

# plot the line y = x * beta1 + beta0, for x in [0, 10]
xvals <- seq(0, 10, 0.01)
yvals <- beta0 + beta1*xvals
plot(xvals, 
     yvals, 
     type = "l", 
     xlab = "x", ylab = "y", col = "red")
```

Lets draw data according to the linear regression model. What is the generative model?

```{r}
#################
# draw data according to the linear regression model. 
#################
n_obs <- 100 # number of observations 
sig <- 6 # std error of the y's
x <- runif(n_obs, 0, 10) # draw x's: they will be fixed for our experiment

draw_data <- function(x, beta0, beta1, sig){
  n_obs <- length(x)
  
  # draw y's 
  y = rnorm(n_obs, mean = beta0 + beta1*x, sd = sig) 
  
  return(data.frame(x = x, y = y))
}

dataset <- draw_data(x, beta0, beta1, sig)

# plot the data 
ggplot() + 
      geom_point(data = dataset, aes(x = x, y = y)) + 
      blank_theme + 
      # this is the line we wish to recover (in red): 
      geom_line(aes(x = xvals, y = yvals), color = 'red') + 
      # this is the estimated regression line (in blue): 
      geom_smooth(data = dataset, 
                  aes(x = x, y = y), method = "lm", 
                        col = "blue", se = FALSE)

```

Lets compute the regression coefficients using our simulated dataset: 
```{r}
# Compute the regression coefficients from simulated data: 
lm(y ~ x, data = dataset)
```
Is it unbiased? We can run a simulation to check: 
```{r}
# draw multiple datasets from the model
# and for each dataset, recompute the regression coefficients. 

n_trials <- 200

intercepts <- rep(0, n_trials)
slopes <- rep(0, n_trials)
for(i in 1:n_trials){
  dataset <- draw_data(x, beta0, beta1, sig)
  
  slm <- lm(y ~ x, data = dataset)
  
  intercepts[i] <- slm$coefficients[1]
  slopes[i] <- slm$coefficients[2]
}
```

What are the distributions of our estimated beta1 and beta0s?
```{r}
# examine distribution of the intercepts
ggplot() + geom_histogram(aes(x = intercepts), 
                          color = 'blue', fill = 'light blue') + 
  blank_theme + 
  # this is the true intercept
  geom_vline(xintercept = beta0, color = 'red')
```

```{r}
# examine distribution of the slopes
ggplot() + geom_histogram(aes(x = slopes), 
                          color = 'blue', fill = 'light blue') + 
  blank_theme + 
  # this is the true slope
  geom_vline(xintercept = beta1, color = 'red')
```
Do our estimates appear unbiased? What can we say about the variance of the estimates? Do they agree with the formulae discussed in class?
