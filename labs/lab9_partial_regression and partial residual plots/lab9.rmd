---
title: "Stat 151A Lab 9"
author: "Stephanie DeGraaf"
date: "October 19, 2018"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE, 
  results = 'markup', dev='png', dpi=150
)
```

## Regression Diagnostics

### Duncan prestige data
Refresher from last week: The data contains a "prestige" score, and we will regress "prestige" on two other covariates: education and income. We looked at leverage scores and identified potential outliers.
```{r}
library(car)
data(Duncan)
head(Duncan)
```

```{r}
mod <- lm(prestige ~ income + education, data=Duncan)
X <- cbind(1, Duncan[,c("income", "education")])
X <- unname(as.matrix(X))
# compute the hat values
H <- X %*% solve(t(X) %*% X) %*% t(X)
lev <- diag(H)
n = length(lev); p = 2
# get the values with high leverage
lev.sorted <- sort(lev, decreasing=T, index.return=T)
```

```{r}
plot(1:n, lev)
for(i in lev.sorted$ix[1:3]) {
  text(i, lev[i]-0.02, rownames(Duncan)[i])
}
hbar <- (p+1)/n
abline(h=2*hbar, lty=2)
abline(h=3*hbar, lty=2)
```

## Cook's Distance
Cook's distance measures influence by how much $\hat \beta$ differs from $\hat \beta_{[i]}$. Essentially, it summarizes how much all of the fitted values change when the ith observation is deleted. A data point having a large $C_i$ indicates that the data point strongly influences the fitted values.

What does this statistic look like? In lecture: we might try $||\hat \beta - \hat \beta_{[i]}||^2$? But this ignores the correlation structure of $\hat \beta$. Correct this by using Mahalanobis distance instead.

\[C_i = \frac{(\hat \beta - \hat \beta_{[i]})^T(X^TX)(\hat \beta - \hat \beta_{[i]})}{(p+1)\hat{\sigma}^2}. \]

We can also write this as
\[C_i=\frac{r_i^2}{p+1} \frac{h_i}{1−h_i}.\]

So we can see that Cook's distance takes into account both the residual and leverage.

**Exercise:** Prove that holding $r_i$ constant, Cook's distance is monotonic increasing in $h_i$.

```{r}
cooks<- cooks.distance(model = mod)
plot(cooks)
cooks.sorted <- sort(cooks, decreasing=T, index.return=T)
for(i in cooks.sorted$ix[1:2]) {
  text(i, cooks[i]-0.02, rownames(Duncan)[i])
}
```

What's a reasonable threshold for a large Cook's distance? The Cook's distance statistic can be formulated in the style of an F-statistic, although it does not follow an F-distribution. However, this suggests that the median point of this distribution, $F_{p,n-p}^{0.5}$, would be a good cutoff:

```{r}
qf(0.5, p, n-p)
```

For large $n$, this value is near 1, so this is another reasonable threshold to identify potentially influential points.

Others suggest $4/n$, or 3 times the mean as cutoffs.

Our textbook author John Fox is rather cautious when it comes to giving strict numerical thresholds. He advises the use of graphics and to examine in closer details the points with "values of D that are substantially larger than the rest".

## Residuals

**Predicted residuals:** the $i$th predicted residual is
\[\hat e_{[i]} = y_i - x_i^T \hat \beta_{[i]}\]
where $\hat \beta_{[i]}$ is the least squares coefficients vector from using the reduced dataset $y_{[i]}$ and $X_{[i]}$ obtained by deleting the ith entry (removing the $i$th observation in the dataset). 

**Exercise:** Show that $\hat e_{[i]} = \hat e_i / (1-h_{ii})$.

Hint: The Woodbury matrix formula allows us to write $\hat \beta_{[i]}$ as $\hat \beta_{[i]} = \hat \beta - \frac{(X^TX)^{-1} \hat e_i}{1-h_{ii}}$.

**Exercise:** What is the variance of $\hat e_{[i]}$?

**Studentized predicted residuals**

We would like to determine if a point is an outlier based on its predicted residual. However, we have just shown that each $e_{[i]}$ has a different variance. Thus, we standardize these residuals to get studentized predicted residuals:
\[t_i = \frac{\hat e_{[i]} \sqrt{1-h_{ii}}}{RSS_{[i]}/(n-p-2)}.\]

**Exercises: True/False**

a) If an observation is an outlier in our dataset, it will have a large residual when we fit a linear regression.

b) If $h_{ii}$ is large, then the predicted residual $\hat e_{[i]}$ has high variance.

c) If the normality assumption is violated, the standardized predicted residuals will not follow a $t$-distribution.

d) Suppose the standardized predicted residuals are $t_1, ..., t_n$ for $n = 30$. Assuming the null hypothesis that all $t_i = 0$, there is a 0.05 probability that some of the $t_i$ will be declared significant by chance alone.

## Partial regression plots

Consider the model $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + e$.

We would like to understand the relationship between $y$ and $x_1$, particularly if it is linear or not. In simple linear regression, we could plot $y \sim x_1$. If we have multiple variables, does this approach still work? 

How could plotting $y \sim x_1$ be misleading? What would be a worst-case scenario?

Partial regression plots (or added variable plots):

For each subject $i=1,...,n$ we have a response variable $y_i$ and explanatory variables $x_{i1},...,x_{ip}$. We can do a least squares fit to get coefficients $\hat \beta_0,...,\hat \beta_p$ that give fitted values
$\hat y_i = \hat \beta_0 + \hat \beta_1 x_{i1} + \dots + \hat \beta_p x_{ip}$.

A partial regression plot is created by doing the following three regressions. (Note here $x_j$ denotes columns of X.)

a) Regress $y$ onto all variables except the first (columns $x_2,...,x_p$) to get fitted values $\tilde{\hat y}$ and residuals $y - \tilde{\hat y}$.

b) Regress the first variable's column $x_1$ on the other variables (columns $x_2,...,x_p$) to get fitted values $\hat x_1$ and residuals $x_1−\hat x_1$.

c) Do a simple regression of the residuals $y-\tilde{\hat y}$ onto the residuals $x - \hat x_1$.

What is the intuition behind this process? Explain in words what is each step doing.

Use the prestige data to make added variable plots for the income and education variables. Compare the added variable plots to scatterplots of prestige by income and prestige by education (without controlling for the other).
```{r}
par(mfrow = c(1,2))
plot(Duncan$income, Duncan$prestige)
plot(Duncan$education, Duncan$prestige)
avPlot(mod, "income")
avPlot(mod, "education")

# they are same
# a = lm(Duncan$prestige ~ Duncan$education)
# b = lm(Duncan$income ~ Duncan$education)
# plot(a$residuals ~ b$residuals)
```

How can we interpret an added variable plot?

We have the following interesting properties:

1) The slope from the simple regression (c) is precisely $\hat \beta_1$ from the original multiple regression.

2) The residuals from the simple regression (c) are the same as the residuals from the original multiple regression.

3) Useful for detecting points of high leverage and/or high influence.

4) Since the residuals are the same, may be good for identifying violations of model assumptions (heteroscedasticity, nonlinearity, etc.)

**Exercise:**
True/False: the regression line for an added variable plot always passes through the origin.

## Partial residual plots

Partial residual plots (aka component plus residual plots) are useful for detecting nonlinearity. 

We already know how to plot added variable plots and regular residual plots. Why do we need partial residual plots?

Partial residual plots offer a useful alternative. (But they are not as suitable for analyzing leverage and influence on coefficients.)

We define the partial residuals for the jth explanatory variable by
\[e_i^{(j)}=e_i+\hat\beta_jx_{ij}=[y_i−(\hat \beta_0+\hat \beta_1x_{i1}+\dots+\hat \beta_p x_{ip})]+ \hat \beta_j x_{ij}.\]
This is just the residual plus the linear component added back.

The component plus residual plot is the plot of $e^{(j)}$ vs $x_j$.

### Detecting Nonlinearity

We've discussed previously that some variables like education or years of experience or other "life-cycle"-type variables might not have a linear relationship with their response variable. Let's see whether we should use a quadratic term to model education instead of a linear term.
```{r}
par(mfrow = c(1,2))
crPlot(mod, variable = "income")
crPlot(mod, variable = "education")

mod2<- lm(prestige ~ income + I(education^2), data = Duncan)
crPlot(mod2, variable = "income")
crPlot(mod2, variable = "I(education^2)")
```

**Recap:** What is the difference between partial regression plots and partial residual plots, and when should we use each plot?
