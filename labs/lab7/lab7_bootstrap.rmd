---
title: "Stat 151A Lab 7"
author: "Stephanie DeGraaf"
date: "October 5, 2018"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
urlcolor: blue
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE, 
  results = 'markup', dev='png', dpi=150
)
```

## Bootstrap Resampling
Recall the linear model setup: 
\[Y = X\beta + e. \]
Our only assumptions for the linear model are that the errors $e_i$ are iid with mean zero and common variance $\sigma^2$.

Importantly, we do not assume normality of the errors. Adding the assumption of normality allows us to conduct statistical significance tests based on a normal distribution. 

However, we can also conduct tests of significance without assuming normality. The bootstrap method allows us to do this.

### Bootstrapping Residuals
To use the bootstrap, we assume the errors come from some arbitrary distribution $F$ with $\mathbb{E}(e_i) = 0$. We use the bootstrap with empirical distribution $\hat F$ as the distribution of $e_i$.

1) Fit a regression on the data to obtain $\hat y = X\hat \beta$ and $\hat e = y - \hat y$.

2) Sample $n$ residuals $e^* = (e_1^*, ... , e_n^*)$ with replacement from the original residuals $\hat e = (\hat e_1, ..., \hat e_n)$. 

3) create a new response vector using the new residuals: $y^* = \hat y + e^*$.

4) Fit a new regression using the new response vector $y^*$ to generate a new estimate of the coefficients $\hat \beta$.

5) Repeat steps 2-4 many times to get a bootstrap distribution of $\hat \beta$.

### Example: GPA dataset

Consider a dataset studying the college GPAS of 141 MSU students from 1994. Suppose we are interested in studying the effect of skipping lectures on GPA. 

Model college GPA by high school GPA and skipped (avg number of lectures missed per week).
```{r}
library(wooldridge)
data("gpa1")
head(gpa1)

model<- lm(colGPA ~ hsGPA + skipped, data = gpa1)
summary(model)
```

```{r}
residuals<- model$residuals
yhat<- model$fitted.values
coefs<- model$coefficients
n<- nrow(gpa1)

B = 5000
results<- matrix(NA, nrow = B, ncol = length(coefs))
for (rep in 1:B){
  boot.resid<- sample(residuals, n, replace = T)
  gpa1$colGPA_boot<- yhat + boot.resid
  boot.model<- lm(colGPA_boot ~ hsGPA + skipped, data = gpa1)
  boot.coefs<- boot.model$coefficients
  results[rep,]<- boot.coefs
}
```

Plot the results:
```{r}
hist(results[,3], main = expression(paste("Histogram of ", hat(beta)[2], " (skipped)")))
abline(v=coefs[3], col = "red")

hist(results[,2], main = expression(paste("Histogram of ", hat(beta)[1], " (hsGPA)")))
abline(v=coefs[2], col = "red")
```

### Bootstrap confidence intervals
There are a few ways to create a confidence interval:

Percentile Confidence Interval: use the 0.025 and 0.975 percentiles of the bootstrap distribution
```{r}
lwr<- quantile(results[,3], 0.025)
upr<- quantile(results[,3], 0.975)
c(lwr, upr)
```

Bootstrap Confidence Interval: estimate the standard error of $\hat \beta$ by the standard deviation of $\hat \beta^{(j)} - \hat \beta$
```{r}
se<- sd(results[,3] - coefs[3])
lwr<- coefs[3] - 1.96*se
upr<- coefs[3] + 1.96*se
c(lwr, upr)
```


## Midterm Review
See practice problems.

## Homework 3
Discuss lingering problems.