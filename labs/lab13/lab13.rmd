---
title: "Stat 151A Lab 13"
author: "Stephanie DeGraaf"
date: "November 30, 2018"
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  cache=TRUE, warning=FALSE, error=FALSE, message=FALSE, echo=TRUE, 
  results = 'markup', dev='png', dpi=150
)
```

## Classification Trees

In a tree estimator, you can think of your dataset as a crowd of people that start at the top of the tree and walk downward. At each node is a sign telling people to go right or left depending on their characteristics (e.g., go left if you are under 18 years old, otherwise go right).

In the case of classification, you have some unknown categorical variable (not necessarily binary), say, “favorite color,” and you hope that at the bottom of the tree, each terminal node has a group of people that mostly like the same color. In other words, the broad goal is to choose good splitting criteria at each node so that the “purity” of the groups increases after the split.

In R, we can construct classification trees using the `rpart` package.

### Impurity measures
How do we measure impurity at a node $G$ [before a split]? Suppose the response variable has $1, ..., C$ categories, and the proportion of the $n_G$ people [at this node $G$] in each category are $p_1, ..., p_C$ respectively.

The impurity of the group of datapoints at this node is
\[I(G)=f(p_1)+f(p_2)+\dots+f(p_C),\]
for some concave function $f$ with $f(0)=f(1)=0$. (This latter condition ensures $I(G)=0$ if G is pure.)

In lecture, we only considered binary classification (label 0 or 1). There, if $p$ denotes the proportion of the $n_G$ people with label 1, the impurity is simply
\[I(G)=f(p)+f(1−p).\]

Examples of $f$:

* Gini index: $f(p)=p(1−p)$. In general, $I(G)=\sum_{i=1}^C p_i(1−p_i) = 1−\sum_{i=1}^C p_i^2$. In binary classification, $I(G)=2p(1−p)$.

* Cross-entropy, deviance, information index: $f(p)= -p\log p$. In general, $I(G)= -\sum_{i=1}^C p_i\log p_i.$ In binary classification, $I(G)= -p\log p - (1−p)\log(1−p).$

* Misclassification error: $f(p)=\min\{p,1−p\}$. In general, $I(G)=\sum_{i=1}^C \min\{p_i,1−p_i\}$. In binary classification, $I(G)=2\min\{p,1−p\}$.

Suppose we split $G$ into two groups $G_L$ and $G_R$ of size $n_L$ and $n_R$ respectively. The impurity reduction of this split is
\[\Delta I:=\frac{1}{n}[n_GI(G)−n_LI(G_L)−n_RI(G_R)],\]
where $n$ is the number of datapoints in the full dataset. We want to choose a split (i.e., choose a variable $X_j$ and a cutoff $c$) that maximizes this impurity reduction, which is equivalent to minimizing
\[n_LI(G_L)+n_RI(G_R),\]
where $I(G)$ is an impurity measure defined above.

By default, `rpart` uses the Gini index; there is also an option to use entropy. However, misclassification error has drawbacks as a splitting criterion and is not an option in `rpart`.

### Stopping and pruning

We have discussed how to choose splits, but when do we stop?

For a nonnegative complexity criterion $\alpha$ (denoted as `cp` in `rpart`), we define the cost complexity of a tree $T$ by
\[C_\alpha(T):=C(T)+\alpha C(T_{root})|T|,\]
where $C(⋅)$ returns some notion of error/risk of a tree, and $T_{root}$ is the tree with no splits (just root node).

* For regression trees, we use $C(T)=RSS(T)$ and thus $C(T_{root})=TSS$, which gives the formula in your lecture notes.

* For classification trees, we use $C(T)= \#errors/n$.

`rpart` has various stopping criteria. See `help(rpart.control)` for some of these parameters.

* `minsplit` (default is 20): The minimum number of observations that must exist in a node in order for a split to be attempted. This parameter can save computation time, since smaller nodes are almost always pruned away by cross validation.

* `minbucket` (default is minsplit/3): The minimum number of observations in a terminal node.

* `cp` (default is 0.01): The threshold complexity parameter.

`rpart` continues splitting until either the `minsplit` or `minbucket` conditions are violated, or if a split does not decrease the cost $C_\alpha$ of the tree. Specifically, suppose $T$ is a tree and $T'$ is a tree with one more split that we are considering. Then
\[C_\alpha(T)−C_\alpha(T')=[C(T)−C(T')]−\alpha C(T_{root}).\]
This gives a nice interpretation of $\alpha$: `rpart` will not consider a split if $C(T)−C(T')$ (the decrease in error) is smaller than $\alpha C(T_{root})$. Thus if we use $\alpha=0$, then `rpart` will construct the entire tree (up to the `minsplit` and `minbucket` conditions.) If we use $\alpha=1$, then `rpart` will not split beyond the root.

After calling `rpart` with some fixed value of $\alpha$ (a.k.a. `cp`, default value is 0.01), we obtain some tree. 

Are we done? Not quite. Trees are prone to overfitting, so we need one more step: pruning. To prune our tree, we want to find the best sub-tree of our tree created by a given $\alpha$.

How can we prune with `rpart`? If we use `printcp()`, it shows what happens if we consider sub-trees that result from using values of $\alpha$ larger than whatever the `cp` value was in our original call to `rpart()`, along with cross-validation scores for each value of $\alpha$. [Recall that larger values of $\alpha$ yield smaller trees.] As you continuously increase $\alpha$ to 1, you will get a finite number of sub-trees, and `printcp()` lists one value of $\alpha$ for each sub-tree. You can then use `prune()` to produce a sub-tree using one of the other values of $\alpha$.

### Example
Let's build a classification tree on a spam email dataset, consisting of 4601 emails, of which 1813 were identified as spam.

```{r}
library(dplyr)
library(DAAG)
data(spam7)
spam = spam7
library(rpart)
sprt = rpart(yesno ~ ., method = "class", data = spam)
plot(sprt, margin=0.1)
text(sprt)
sprt
```

As mentioned earlier, the default call to `rpart` uses Gini. To use entropy, you need to pass another argument to the `rpart` function as: 
`rpart(yesno ~ ., method = "class", parms = list(split = 'information'), data = spam)`.

How can we measure how good this tree is? We have two measures, precision and recall that are both based on the confusion matrix:

```{r, echo = F}
library(kableExtra)
cm<- data.frame("Predict0" = c("a", "c"), "Predict1"= c("b", "d"))
rownames(cm)<- c("Actual0", "Actual1")
kable(cm) %>% kable_styling(full_width = F)
```

Precision is $\frac{d}{b+d}$. Recall is $\frac{d}{c+d}$.

```{r}
confusion <- function (y, yhat){
  a <- sum((!y) & (yhat<=0.5))
  b <- sum((!y) & (yhat>0.5))
  c <- sum((y) & (yhat<=0.5)) 
  d <- sum((y) & (yhat>0.5))
  conf <- matrix(c(a,b,c,d), ncol = 4)
  colnames(conf) <- c("a","b","c","d")
  return(conf)
}
y.tr = predict(sprt, spam)[,2]
y = as.numeric(spam$yesno == "y")
conf = confusion(y, y.tr)
precision <- conf[4] / (conf[2] + conf[4])
recall <- conf[4] / (conf[3] + conf[4])
c(precision, recall)
```

Instead of using the default, we can pick the $\alpha$ value by cross-validation.

```{r}
set.seed(0)
# pick a small value of alpha to build a large tree
dense.sprt <- rpart(yesno ~ ., method="class", data=spam, cp=0.001)
plot(dense.sprt, margin=0.1)
text(dense.sprt)
table.cp <- printcp(dense.sprt)
cp.x <- table.cp[which.min(table.cp[,4]), 1]
cp.x

# prune our large tree with the cv-chosen alpha
fsprt <- prune(dense.sprt, cp.x)
plot(fsprt, margin=0.1)
text(fsprt)
fconf <- table(spam$yesno, as.numeric(predict(fsprt)[,2] >= 0.5))
fconf

# determine the precision and recall of our pruned tree
fconf <- as.vector(t(fconf))
fprecision <- fconf[4] / (fconf[2] + fconf[4])
frecall <- fconf[4] / (fconf[3] + fconf[4])
c(fprecision, frecall)
# compare to previous tree using default alpha:
c(precision, recall)
```

Why do we need classification trees if we already have logistic regression?

Both methods can be used to classify observations into groups and predict the group of a new observation. The difference is in the geometry:

* Logistic regression finds a single linear decision boundary in our feature space.

* Classification trees partition our feature space into smaller half-spaces using axis-aligned linear decision boundaries. The resulting decision boundary will be non-linear and there could be multiple boundaries.

How do we decide which method to use?

* Classification trees are appropriate when the data points aren't easily separated by a single hyperplane. 

* On the other hand, decisions trees are so flexible that they can be prone to overfitting. (To combat this, we prune. But in general logistic regression tends to be less susceptible (although not immune) to overfitting.)

* Decision trees can automatically take into account interactions between variables, e.g. xy if you have two independent features x and y. With logistic regression, you'll have to manually add those interaction terms yourself.

* In practice, it's easy to just try both models and test them using cross-validation. This will help you find out which one is more likely to have better generalization error.

## Shrinkage

Shrinkage methods adjust our usual model fitting process to select only a subset of the available explanatory variables, in order to improve prediction accuracy and interpretability. 

The lasso forces the sum of the absolute value of the regression coefficients to be less than some fixed value, which forces certain coefficients to be zero. This has the effect of doing variable selection and giving us a smaller, more interpretable model, while improving prediction accuracy.

The `glmnet` package in R can be used to apply shrinkage methods to linear models. This solves the minimization problem
\[\hat \beta = \arg \min_{\beta} ||y - X\beta||^2 + \lambda ||\beta||_1. \]

### Logistic regression with LASSO

As suggested by the name of the package, `glmnet` can also do regularization with GLM models. We simply replace the least squares term $||y−X\beta||^2$ with the negative log likelihood of whichever GLM we want.

In the case of logistic regression, it is
\[\min_{\beta_0,\beta} −\left[\frac{1}{n} \sum_{i=1}^n y_i(\beta_0+x_i^T\beta)−\log(1+\exp(\beta_0+x_i^T\beta))\right]+\lambda ||\beta||_1.\]

We can ask `glmnet` to do this by adding `family="binomial"` to the call. 

### Example

Consider the Titanic dataset. We would like to predict survival using logistic regression with lasso shrinkage.

```{r}
library(glmnet)
titanic <- read.csv("titanic_train.csv")
titanic <- na.omit(titanic)

# need to use model.matrix() to create design matrix for glmnet
dd <- titanic[, c("Survived", "Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked")]
dd$Pclass <- as.factor(dd$Pclass)
X <- model.matrix(Survived~., dd)
X <- X[,-1]

# in glmnet, alpha = 1 is lasso, alpha = 0 is ridge
fit <- glmnet(X, titanic$Survived, family="binomial", alpha = 1)
plot(fit, xvar="lambda")
cvfit <- cv.glmnet(X, titanic$Survived, family="binomial")
plot(cvfit)
#lambda.min is the value of λ that gives minimum mean cross-validated error
#lambda.1se gives the most regularized model such that error is within one standard error of the minimum
coef(cvfit, s=cvfit$lambda.1se)

pred.survival<- as.numeric(predict(cvfit, newx = X, type = "response", s = "lambda.1se"))
true.survival<- titanic$Survived
conf = confusion(true.survival, pred.survival)
precision <- conf[4] / (conf[2] + conf[4])
recall <- conf[4] / (conf[3] + conf[4])
c(precision, recall)
```

For more details, the [glmnet vignette](https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html) is very helpful.

## Midterm 2 Solutions

